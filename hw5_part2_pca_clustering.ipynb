{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "4e6cefb0049d48a2f4648d752841bb06",
     "grade": false,
     "grade_id": "cell-b038e38b5e3072a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Prepared by: Prof Daniel E Acuna <deacuna@syr.edu>\n",
    "- Modified by: Prof Humayun Khan <hhkhan@syr.edu>\n",
    "- Faculty Assistant: Eashani Deorukhkar <edeorukh@syr.edu>\n",
    "- Faculty Assistant: Yash Kapadia <ykapadia@syr.edu>\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers either from your classmates or from the internet__\n",
    "- You can put the homework files anywhere you want in your http://notebook.acuna.io workspace but _do not change_ the file names. The TAs and the professor use these names to grade your homework.\n",
    "- Remove or comment out code that contains `raise NotImplementedError`. This is mainly to make the `assert` statement fail if nothing is submitted.\n",
    "- The tests shown in some cells (i.e., `assert` and `np.testing.` statements) are used to grade your answers. **However, the professor and TAs will use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before downloading and submitting your work through Blackboard, remember to save and press `Validate` (or go to \n",
    "`Kernel`$\\rightarrow$`Restart and Run All`). \n",
    "- Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load these packages\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml import clustering\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql import functions as fn\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Clustering and Latent Dirichlet Allocation\n",
    "\n",
    "I would recommend to follow the notebook `unsupervised_learning.ipynb` first, shared through the IST 718 repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataset contains information about the diet of European contries around 1970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9102f93b73342f27ff5eb15d025c29c1",
     "grade": false,
     "grade_id": "cell-8898c04579b9221e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "protein_df = spark.read.csv('proteindata.txt', sep='\\t', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=protein_df.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: (10 pts)\n",
    "\n",
    "Performed a PCA transformation on the `protein_df` dataframe, using all features. Use two principal components. The pipeline transformation should have a standard scaler step where you only center the data before you pass it onto the PCA transformation. Store the pipeline transformation in `pipe_pca`. The transformation should produce a column named `pc` which has the two principal components requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cb982914cbab5d8d552cafcab8a6f41b",
     "grade": false,
     "grade_id": "cell-18ebd4f77c37a8e3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "pipe_pca=Pipeline(stages=[\n",
    "    feature.VectorAssembler(inputCols=col,outputCol='features'),\n",
    "    feature.StandardScaler(withStd=False, withMean=True, inputCol='features', outputCol='centered_features'),\n",
    "    feature.PCA(k=2,inputCol='centered_features',outputCol='pc')\n",
    "]).fit(protein_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_df=pipe_pca.transform(protein_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- RedMeat: double (nullable = true)\n",
      " |-- WhiteMeat: double (nullable = true)\n",
      " |-- Eggs: double (nullable = true)\n",
      " |-- Milk: double (nullable = true)\n",
      " |-- Fish: double (nullable = true)\n",
      " |-- Cereals: double (nullable = true)\n",
      " |-- Starch: double (nullable = true)\n",
      " |-- Nuts: double (nullable = true)\n",
      " |-- FruitVeg: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- centered_features: vector (nullable = true)\n",
      " |-- pc: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the schema of how the transformation should look like:\n",
    "\n",
    "```python\n",
    "pipe_pca.transform(protein_df).printSchema()\n",
    "```\n",
    "\n",
    "```console\n",
    "root\n",
    " |-- Country: string (nullable = true)\n",
    " |-- RedMeat: double (nullable = true)\n",
    " |-- WhiteMeat: double (nullable = true)\n",
    " |-- Eggs: double (nullable = true)\n",
    " |-- Milk: double (nullable = true)\n",
    " |-- Fish: double (nullable = true)\n",
    " |-- Cereals: double (nullable = true)\n",
    " |-- Starch: double (nullable = true)\n",
    " |-- Nuts: double (nullable = true)\n",
    " |-- FruitVeg: double (nullable = true)\n",
    " |-- features: vector (nullable = true)\n",
    " |-- centered_features: vector (nullable = true)\n",
    " |-- pc: vector (nullable = true)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "45e0f0dc7c75b0c288bbd0f818803065",
     "grade": true,
     "grade_id": "cell-20fb7c865c3ddf0e",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (10 pts)\n",
    "np.testing.assert_equal(pipe_pca.transform(protein_df).count(), 25)\n",
    "np.testing.assert_equal(type(pipe_pca), PipelineModel)\n",
    "np.testing.assert_equal(len(pipe_pca.transform(protein_df).first().pc), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (10 pts)\n",
    "\n",
    "Extract the absolute loadings from the PCA transformation in the pipeline and the appropriate feature names from the vector assembler. For each principal component, extract the top feature---the biggest absoluate loadings. Comment on what principal component 1 vs principal component 2 mean based on these top features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "59aa36c0b782325c43cd3225a6363e6e",
     "grade": true,
     "grade_id": "cell-ccff5d0a482e4302",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>pca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fish</td>\n",
       "      <td>0.860865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature       pca\n",
       "5    Fish  0.860865"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_model=pipe_pca.stages[2]\n",
    "pc1 = pca_model.pc.toArray()[:, 0].tolist()\n",
    "pc2 = pca_model.pc.toArray()[:, 1].tolist()\n",
    "features=transformed_df.columns[:9]\n",
    "a=pd.DataFrame(data=[features,pc1]).T.rename(columns={0:'feature',1:'pca'})\n",
    "a['pca']=a['pca'].apply(np.abs)\n",
    "a=a.sort_values(by='pca',ascending=False).head(1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>pca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eggs</td>\n",
       "      <td>0.830856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature       pca\n",
       "3    Eggs  0.830856"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=pd.DataFrame(data=[features,pc2]).T.rename(columns={0:'feature',1:'pca'})\n",
    "b['pca']=b['pca'].apply(np.abs)\n",
    "b=b.sort_values(by='pca',ascending=False).head(1)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 (10 pts)\n",
    "\n",
    "Use the following `exploded_df` dataframe to explore which countries are at the extreme the principal component 1. In particular, create a dataframe `smallest_pc1_df` which contains the columns `country`, `pc_1`, and `pc_2` of the country with the smallest `pc_1`. Similarly, create `largest_pc1_df` with the largest `pc_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1ef2748ac9694e621534f919c4698390",
     "grade": false,
     "grade_id": "cell-ef40fee5c7d7bf9a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "@fn.udf(returnType=types.FloatType())\n",
    "def vector_select(vector_col, element):\n",
    "    return float(vector_col[element])\n",
    "\n",
    "exploded_df = pipe_pca.transform(protein_df).select('country', \n",
    "                                      vector_select('pc', fn.lit(0)).alias('pc_1'),\n",
    "                                      vector_select('pc', fn.lit(1)).alias('pc_2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should contain something similar to this:\n",
    "\n",
    "```\n",
    "exploded_df.show()\n",
    "```\n",
    "\n",
    "```\n",
    "+---------+-----------+------------+\n",
    "|  country|       pc_1|        pc_2|\n",
    "+---------+-----------+------------+\n",
    "|  Albania|  14.102253|  -1.3218285|\n",
    "|  Austria| -5.4612746|   1.5477921|\n",
    "|  Belgium| -6.0766153|  -1.4793622|\n",
    "| Bulgaria|  26.115946|   3.3188622|\n",
    "|Czechosl.|   3.317268|  -2.0923328|\n",
    "|  Denmark|-13.8607855|   1.3738568|\n",
    "| EGermany| -4.9024506|    -8.35954|\n",
    "|  Finland| -12.262294|   11.290117|\n",
    "|   France|  -6.345336|  0.67163473|\n",
    "|   Greece|   9.036383|   3.0326154|\n",
    "|  Hungary|  10.804568|  -2.3634377|\n",
    "|  Ireland| -11.857348|    5.312214|\n",
    "|    Italy|   6.308735|  -1.3141143|\n",
    "| Netherl.| -11.808532|    2.132773|\n",
    "|   Norway| -11.005402|-0.077100314|\n",
    "|   Poland|  2.5261996|   2.9990442|\n",
    "| Portugal| 0.78424096|  -16.753153|\n",
    "|  Romania|  19.067295|   2.5912552|\n",
    "|    Spain|  1.9230922|  -10.482929|\n",
    "|   Sweden|-14.8419485|   0.7262486|\n",
    "+---------+-----------+------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "96d4b13f36029a10d80b2727f2a66dbe",
     "grade": false,
     "grade_id": "cell-e3414da54a6ab4f5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "smallest_pc1_df=exploded_df.sort('pc_1').limit(1)\n",
    "largest_pc1_df=exploded_df.sort(fn.desc('pc_1')).limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6045a3a99e15219280c28400bd802e7c",
     "grade": true,
     "grade_id": "cell-11ee0e6c6a5219e2",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (10 pts)\n",
    "np.testing.assert_equal(smallest_pc1_df.count(), 1)\n",
    "np.testing.assert_equal(largest_pc1_df.count(), 1)\n",
    "np.testing.assert_array_less(smallest_pc1_df.first().pc_1, largest_pc1_df.first().pc_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (10 pts)\n",
    "\n",
    "In this question, you will find three clusters of the raw features (without any modification) of `protein_df` using K-means. Do this using a pipeline transformation called `pipe_cluster` which should produce a prediction column called `prediction`. **To reproduce your results, use default parameters of Kmeans and set the `seed` parameter to 0**\n",
    "\n",
    "Produce a dataframe `protein_clustered_df` with the `pipe_cluster` transformation applied to `protein_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1065593910d638d2c98e2d554eaaabeb",
     "grade": false,
     "grade_id": "cell-2b84d6996a843772",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "pipe_cluster=Pipeline(stages=[\n",
    "    feature.VectorAssembler(inputCols=col,outputCol='features'),\n",
    "    clustering.KMeans(k=3,featuresCol='features',predictionCol='prediction')\n",
    "]).fit(protein_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_clustered_df=pipe_cluster.transform(protein_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dfb9762f17b64f2d9ce5ed591937cc22",
     "grade": true,
     "grade_id": "cell-d352a2be7c9d703f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 10 pts\n",
    "np.testing.assert_equal(type(pipe_cluster), PipelineModel)\n",
    "np.testing.assert_equal(len(pipe_cluster.stages), 2)\n",
    "np.testing.assert_equal(protein_clustered_df.count(), 25)\n",
    "assert 'prediction' in protein_clustered_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (10 pts)\n",
    "\n",
    "According to Wikipedia, the Mediterranean diet includes \"proportionally high consumption of olive oil, legumes, unrefined cereals, fruits, and vegetables, moderate to high consumption of fish, moderate consumption of dairy products (mostly as cheese and yogurt), moderate wine consumption, and low consumption of non-fish meat products.\" https://en.wikipedia.org/wiki/Mediterranean_diet\n",
    "\n",
    "We have some of these countries in our protein dataset: Italy, Spain, Portugal, and Greece.\n",
    "\n",
    "With code and code comments, test the hypothesis that these countries all belong to the same cluster using the clustering you did in question 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use the following list in your code\n",
    "mediterranean_countries = ['Italy', 'Spain', 'Portugal', 'Greece']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_clustered_df.createOrReplaceTempView('temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "415e2f135d2fffb38e3a4f04cb8b4c36",
     "grade": true,
     "grade_id": "cell-9285cef3ce344449",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+----+----+----+-------+------+----+--------+--------------------+----------+\n",
      "| Country|RedMeat|WhiteMeat|Eggs|Milk|Fish|Cereals|Starch|Nuts|FruitVeg|            features|prediction|\n",
      "+--------+-------+---------+----+----+----+-------+------+----+--------+--------------------+----------+\n",
      "|  Greece|   10.2|      3.0| 2.8|17.6| 5.9|   41.7|   2.2| 7.8|     6.5|[10.2,3.0,2.8,17....|         2|\n",
      "|   Italy|    9.0|      5.1| 2.9|13.7| 3.4|   36.8|   2.1| 4.3|     6.7|[9.0,5.1,2.9,13.7...|         2|\n",
      "|Portugal|    6.2|      3.7| 1.1| 4.9|14.2|   27.0|   5.9| 4.7|     7.9|[6.2,3.7,1.1,4.9,...|         2|\n",
      "|   Spain|    7.1|      3.4| 3.1| 8.6| 7.0|   29.2|   5.7| 5.9|     7.2|[7.1,3.4,3.1,8.6,...|         2|\n",
      "+--------+-------+---------+----+----+----+-------+------+----+--------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from temp where Country=\"Italy\" or Country=\"Spain\" or Country=\"Portugal\" or Country=\"Greece\" ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our current prediction all of the above mentioned countires belong to the same cluster. So we can say they are mrediterranean countires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
