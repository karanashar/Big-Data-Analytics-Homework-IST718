{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4e6cefb0049d48a2f4648d752841bb06",
     "grade": false,
     "grade_id": "cell-b038e38b5e3072a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Prepared by: Prof Willard Williamson <wewillia@syr.edu>\n",
    "- Modified by: Prof Humayun Khan <hhkhan@syr.edu>\n",
    "- Faculty Assistant: Eashani Deorukhkar <edeorukh@syr.edu>\n",
    "- Faculty Assistant: Yash Kapadia <ykapadia@syr.edu>\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Code from the class text books or class provided code can be copied in its entirety.__\n",
    "- __Do not change homework file names.__ The FAs and the professor use these names to grade your homework.  Changing file names may result in a point reduction penalty.\n",
    "- There could be tests in some cells (i.e., `assert` and `np.testing.` statements). These tests (if present) are used to grade your answers. **However, the professor and FAs could use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before submitting your work, remember to check for run time errors with the following procedure:\n",
    "`Kernel`$\\rightarrow$`Restart and Run All`.  All runtime errors will result in a minimum penalty of half off.\n",
    "- Data Bricks is the official class runtime environment so you should test your code on Data Bricks before submission.  If there is a runtime problem in the grading environment, we will try your code on Data Bricks before making a final grading decision.\n",
    "- All plots shall include a title, and axis labels.\n",
    "- Grading feedback cells are there for graders to provide feedback to students.  Don't change or remove grading feedback cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name - Karan Puran Ashar\n",
    "   NetID - kashar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark and sparkcontext objects\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only cell\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# get the databricks runtime version\n",
    "db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "\n",
    "# Define a function to read the data file.  The full path data file name is constructed\n",
    "# by checking runtime environment variables to determine if the runtime environment is \n",
    "# databricks, grading, or a student's personal computer.  The full path file name is then\n",
    "# constructed based on the runtime env.\n",
    "#\n",
    "# Notes:\n",
    "#   Graders, set the GRADING_RUNTME_ENV environment variable to contain the full path \n",
    "#   to the data file for this assignment.  For example, my grading_env var is set as \n",
    "#   follows on Windows:\n",
    "#   set GRADING_RUNTIME_ENV=c:/Users/Will/Desktop/SU/IST-718/datasets\n",
    "# \n",
    "# Params\n",
    "#   data_file_name: The base name of the data file to load\n",
    "# \n",
    "# Returns the full path file name based on the runtime env\n",
    "#\n",
    "def get_training_filename(data_file_name):    \n",
    "    # The grading_env variable contains the full path to the \n",
    "    # directory containing the data file.  \n",
    "    grading_env = os.getenv(\"GRADING_RUNTIME_ENV\")\n",
    "    \n",
    "    # if the databricks env var exists\n",
    "    if db_env != None:\n",
    "        # build the full path file name assuming data brick env\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "    # else if the grading environment variable exists\n",
    "    elif grading_env != None:\n",
    "        # build the full path file name assuming a grading env\n",
    "        full_path_name = \"%s/%s\" % (grading_env, data_file_name)\n",
    "    # else the data is assumed to be in the same dir as this notebook\n",
    "    else:\n",
    "        # Assume the student is running on their own computer and load the data\n",
    "        # file from the same dir as this notebook\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "    # return the full path file name to the caller\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning: Do not use Pandas at all in this part of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data cleaning and basic analyses\n",
    "\n",
    "In this part, you will learn to read data from non-standard formats, clean data, and produce some basic analysis of it.\n",
    "\n",
    "We will use Spark 1.6 (`sparkContext` on variable `sc`) to load text files from which we will extract features that are predictive of a target value. Unfortunately, the data is stored in some non-standard format where each line contains the customer index, the feature index, and the value of the feature for that customer. Similarly, the target files contain in each line the customer index and the target value. We will load these files into two RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "52b18c5545e26c712a0ff9f85bf9e755",
     "grade": false,
     "grade_id": "cell-e8f7bc29c04e04e0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Adjust the following lines as needed to read the data files\n",
    "# If running on databricks, you will need to upload the data to databricks and then\n",
    "# adjust the file path as demonstrated in class.\n",
    "raw_features_rdd = sc.textFile(get_training_filename('hw2_dataset_features.txt'))\n",
    "raw_target_rdd = sc.textFile(get_training_filename('hw2_dataset_targets.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue with the data is that there were problems transmitting the features and targets. If this happened, then a text `ERROR` or `ERROR TRANSFERRING` replaced the data. If you look at the first 10 values of the features and target RDD, you will see these types of lines:\n",
    "\n",
    "```python\n",
    "raw_features_rdd.take(10)\n",
    "```\n",
    "\n",
    "```\n",
    "['<customer_feature customer_id=0 feature_id=0>-0.57</customer_feature>',\n",
    " '<customer_feature customer_id=0 feature_id=1>-0.38</customer_feature>',\n",
    " '<customer_feature customer_id=0 feature_id=2>0.00</customer_feature>',\n",
    " '<customer_feature customer_id=0 feature_id=3>-0.07</customer_feature>',\n",
    " '<customer_feature customer_id=0 feature_id=4>-0.28</customer_feature>',\n",
    " '<customer_feature customer_id=0 feature_id=5>-0.79</customer_feature>',\n",
    " 'ERROR',\n",
    " '<customer_feature customer_id=0 feature_id=7>0.28</customer_feature>',\n",
    " '<customer_feature customer_id=0 feature_id=8>1.65</customer_feature>',\n",
    " '<customer_feature customer_id=0 feature_id=9>0.57</customer_feature>']\n",
    "```\n",
    "\n",
    "```python\n",
    "raw_target_rdd.take(35)\n",
    "```\n",
    "\n",
    "```\n",
    "['<customer_target customer_id=0>-252.49</customer_target>',\n",
    " '<customer_target customer_id=1>36.67</customer_target>',\n",
    " '<customer_target customer_id=2>138.02</customer_target>',\n",
    " '<customer_target customer_id=3>-429.54</customer_target>',\n",
    " '<customer_target customer_id=4>-18.23</customer_target>',\n",
    " '<customer_target customer_id=5>-5.52</customer_target>',\n",
    " '<customer_target customer_id=6>-31.96</customer_target>',\n",
    " 'ERROR TRANSFERRING',\n",
    " 'ERROR TRANSFERRING',\n",
    " '<customer_target customer_id=9>-111.88</customer_target>']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<customer_feature customer_id=0 feature_id=0>-0.57</customer_feature>']\n",
      "['<customer_target customer_id=0>-252.49</customer_target>']\n"
     ]
    }
   ],
   "source": [
    "# file read test\n",
    "print(raw_features_rdd.take(1))\n",
    "print(raw_target_rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 (10 pts):\n",
    "\n",
    "Filter out the lines that contain errors and store them in `raw_features2_rdd` and `raw_targets2_rdd` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7258f0792387b2cdd0c4bb92886496ad",
     "grade": false,
     "grade_id": "cell-626aab3709c46ced",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create raw_features2_rdd and raw_targets2_rdd below\n",
    "raw_features2_rdd=raw_features_rdd.filter(lambda x:x!='ERROR')\n",
    "raw_targets2_rdd=raw_target_rdd.filter(lambda x:x!='ERROR TRANSFERRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95036\n",
      "8968\n"
     ]
    }
   ],
   "source": [
    "# check that things work\n",
    "print(raw_features2_rdd.count())\n",
    "print(raw_targets2_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "34b858b2a4dae4c0c6569a235758d7e7",
     "grade": true,
     "grade_id": "cell-16fe9b731c74bdb1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the lines are properly discarded\"\"\"\n",
    "assert raw_features2_rdd.count() == 95036\n",
    "assert raw_targets2_rdd.count() == 8968"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 (10 pts):\n",
    "You will further process `raw_features2_rdd` such that you will create a key-value RDD of the following form: the key is the customer index as an integer and the value is a dictionary whose key is a string `f_0`, `f_1`, ..., `f_9` for feature index 0, 1, ... 9, respetively, and the value is a floating point number of the feature value. \n",
    "\n",
    "Define a function `map_features2` that performs such key-value pair creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1ac97b311e4bedac55e3aee7cf0a78b3",
     "grade": false,
     "grade_id": "cell-6a8bd9ba37f02ccc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def map_features2(line):\n",
    "    string=re.compile('.*=(\\d+).*=(\\d)>(-?\\d+.\\d+)')\n",
    "    a=re.findall(string,line)\n",
    "    customer_id=int(a[0][0])\n",
    "    feature=a[0][1]\n",
    "    dic={}\n",
    "    dic['f_'+feature]=float(a[0][2])\n",
    "    return ((customer_id,dic))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for the input element:\n",
    "\n",
    "`'<customer_feature customer_id=4 feature_id=0>-0.79</customer_feature>'`\n",
    "\n",
    "it should generate\n",
    "```python\n",
    "[4, {'f_0': -0.79}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, {'f_0': -0.57}),\n",
       " (0, {'f_1': -0.38}),\n",
       " (0, {'f_2': 0.0}),\n",
       " (0, {'f_3': -0.07}),\n",
       " (0, {'f_4': -0.28}),\n",
       " (0, {'f_5': -0.79}),\n",
       " (0, {'f_7': 0.28}),\n",
       " (0, {'f_8': 1.65}),\n",
       " (0, {'f_9': 0.57}),\n",
       " (1, {'f_0': 0.5})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it here\n",
    "raw_features2_rdd.\\\n",
    "    map(map_features2).\\\n",
    "    take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d152e2bbb8f36de1ff57cf05b404ff89",
     "grade": true,
     "grade_id": "cell-185fc07f367e8eb6",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the new raw_features2_rdd is correct\"\"\"\n",
    "# key is an integer\n",
    "np.testing.assert_equal(type(raw_features2_rdd.map(map_features2).first()[0]), int)\n",
    "# value is a dictionary\n",
    "np.testing.assert_equal(type(raw_features2_rdd.map(map_features2).first()[1]), dict)\n",
    "np.testing.assert_equal(type([*raw_features2_rdd.map(map_features2).first()[1].values()][0]), float)\n",
    "np.testing.assert_approx_equal(raw_features2_rdd.map(map_features2).map(lambda x: x[1]).map(lambda d: [*d.items()][0][1]).sum(), 137.7099999999997, significant=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3 (10 pts):\n",
    "\n",
    "You will create a function `map_target2` that will be applied to `raw_target2_rdd`. This function will create key-value pair where the key is the customer index as an integer and the value is the floating point representation of the target. Assign the resulting RDD to `raw_target3_rdd`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "35c7e987523d093eba540b64dffb0e78",
     "grade": false,
     "grade_id": "cell-0a9dc80ecf0c1b6a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def map_target2(line):\n",
    "    string=re.compile('.*=(\\d+)>(-?\\d+.\\d{2}).*')\n",
    "    b=re.findall(string,line)\n",
    "    return (int(b[0][0]),float(b[0][1]))\n",
    "    \n",
    "    \n",
    "# make the assignment here\n",
    "raw_target3_rdd = raw_targets2_rdd.map(map_target2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of results:\n",
    "\n",
    "```python\n",
    "raw_target2_rdd.map(map_target2).sortByKey().take(5)\n",
    "```\n",
    "\n",
    "```\n",
    "[(0, -252.49), (1, 36.67), (2, 138.02), (3, -429.54), (4, -18.23)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -252.49), (1, 36.67), (2, 138.02), (3, -429.54), (4, -18.23)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it yourself\n",
    "raw_targets2_rdd.map(map_target2).sortByKey().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3a9dcf95e4643d24fa6987f060f6b479",
     "grade": true,
     "grade_id": "cell-00147973f05ae1e7",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that raw_target3_rdd contains the right values\"\"\"\n",
    "# check types\n",
    "np.testing.assert_equal(type(raw_target3_rdd.keys().first()), int)\n",
    "np.testing.assert_equal(type(raw_target3_rdd.values().first()), float)\n",
    "# the sum of all targets\n",
    "np.testing.assert_approx_equal(raw_target3_rdd.values().sum(), -179351.71, significant=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4 (10 pts):\n",
    "\n",
    "In this question, you will use map reduce to produce an RDD of key-value pairs where the key is the customer index and the value is a dictionairy with all the features and values associated with that customer. Notice that the map part of the map-reduce is already defined by `map_features2` on `raw_features2_rdd`. Therefore, define the proper `reduce_features2` function to produce the desired results. Create a RDD named `raw_features3_rdd` with the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bf88749fa6073bb62b123416b87783c6",
     "grade": false,
     "grade_id": "cell-c51c5ed026c36cb7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def reduce_features2(v1, v2):\n",
    "    dic1={}\n",
    "    dic1.update(v1)\n",
    "    dic1.update(v2)\n",
    "    return dic1\n",
    "# Apply mapreduce to produce the raw_features3_rdd from raw_features2_rdd\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features3_rdd=raw_features2_rdd.map(map_features2).reduceByKey(reduce_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  {'f_0': -0.57,\n",
       "   'f_1': -0.38,\n",
       "   'f_2': 0.0,\n",
       "   'f_3': -0.07,\n",
       "   'f_4': -0.28,\n",
       "   'f_5': -0.79,\n",
       "   'f_7': 0.28,\n",
       "   'f_8': 1.65,\n",
       "   'f_9': 0.57}),\n",
       " (1,\n",
       "  {'f_0': 0.5,\n",
       "   'f_1': 0.8,\n",
       "   'f_2': -0.49,\n",
       "   'f_3': 0.25,\n",
       "   'f_4': 0.37,\n",
       "   'f_5': 0.73,\n",
       "   'f_6': -0.43,\n",
       "   'f_7': 0.89,\n",
       "   'f_8': -1.85,\n",
       "   'f_9': -0.44})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_features3_rdd.sortByKey().take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the map reduce should produce the following example result:\n",
    "```python\n",
    "raw_features3_rdd.sortByKey().take(2)\n",
    "```\n",
    "\n",
    "```console\n",
    "[(0,\n",
    "  {'f_0': -0.57,\n",
    "   'f_1': -0.38,\n",
    "   'f_2': 0.0,\n",
    "   'f_3': -0.07,\n",
    "   'f_4': -0.28,\n",
    "   'f_5': -0.79,\n",
    "   'f_7': 0.28,\n",
    "   'f_8': 1.65,\n",
    "   'f_9': 0.57}),\n",
    " (1,\n",
    "  {'f_0': 0.5,\n",
    "   'f_1': 0.8,\n",
    "   'f_2': -0.49,\n",
    "   'f_3': 0.25,\n",
    "   'f_4': 0.37,\n",
    "   'f_5': 0.73,\n",
    "   'f_6': -0.43,\n",
    "   'f_7': 0.89,\n",
    "   'f_8': -1.85,\n",
    "   'f_9': -0.44})]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ed95bb2f5c17ad9f9fd2528d2a551730",
     "grade": true,
     "grade_id": "cell-f74bab4029291e96",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Check that raw_features3_rdd has the correct format and values. There could be hidden tests!\"\"\"\n",
    "# key is an integer\n",
    "np.testing.assert_equal(type(raw_features3_rdd.first()[0]), int)\n",
    "# value is a dictionary\n",
    "np.testing.assert_equal(type(raw_features3_rdd.first()[1]), dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.5 (10 pts):\n",
    "\n",
    "Join the two RDDs `raw_target3_rdd` and `raw_features3_rdd` to create an RDD with elements of the form\n",
    "\n",
    "`[customer_index, (target, feature_dict)]`\n",
    "\n",
    "where `target` comes from `raw_target3_rdd`, and `feature_dict` is the dictionary with features from `raw_features3_rdd`.\n",
    "\n",
    "Assign to `rdd_complete` a filtered version of the join for customers who have all 10 features and a target.\n",
    "\n",
    "Finally use `rdd_complete_rows` to create an RDD of `Row` objects where you map each entry to the following format:\n",
    "\n",
    "`Row(customer_index,  f_0,  f_1,  f_2,  f_3,  f_4,  f_5,  f_6,  f_7,  f_8,  f_9, target)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, from `rdd_complete`:\n",
    "\n",
    "```python\n",
    "rdd_complete.sortByKey().first()\n",
    "```\n",
    "should return\n",
    "```\n",
    "(1,\n",
    " (36.67,\n",
    "  {'f_0': 0.5,\n",
    "   'f_1': 0.8,\n",
    "   'f_2': -0.49,\n",
    "   'f_3': 0.25,\n",
    "   'f_4': 0.37,\n",
    "   'f_5': 0.73,\n",
    "   'f_6': -0.43,\n",
    "   'f_7': 0.89,\n",
    "   'f_8': -1.85,\n",
    "   'f_9': -0.44}))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "758a59186ed17064075d698f947a4784",
     "grade": false,
     "grade_id": "cell-225a18b6658c1598",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (-252.49,\n",
       "   {'f_0': -0.57,\n",
       "    'f_1': -0.38,\n",
       "    'f_2': 0.0,\n",
       "    'f_3': -0.07,\n",
       "    'f_4': -0.28,\n",
       "    'f_5': -0.79,\n",
       "    'f_7': 0.28,\n",
       "    'f_8': 1.65,\n",
       "    'f_9': 0.57}))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_complete=raw_target3_rdd.join(raw_features3_rdd)\n",
    "rdd_complete.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_complete=rdd_complete.filter(lambda x:len(x[1][1])==10).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "57ff1356246ff36cf7c81d5fe8b33b87",
     "grade": true,
     "grade_id": "cell-47b082a8f8d07296",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Test if `rdd1` has the right data. Remember that there could be hidden tests!\"\"\"\n",
    "# number of elements expected\n",
    "np.testing.assert_equal(rdd_complete.count(), 5379)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e518b98058f857ef947f11afe2774230",
     "grade": false,
     "grade_id": "cell-63a9f734c2e8d9f1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# here define the function to_row that a Row object for an entry of rdd_complete\n",
    "def to_row(e):\n",
    "    row=Row(customer_index=e[0],f_0=e[1][1].get('f_0'),f_1=e[1][1].get('f_1'),f_2=e[1][1].get('f_2'),f_3=e[1][1].get('f_3'),f_4=e[1][1].get('f_4'),f_5=e[1][1].get('f_5'),f_6=e[1][1].get('f_6'),f_7=e[1][1].get('f_7'),f_8=e[1][1].get('f_8'),f_9=e[1][1].get('f_9'),target=e[1][0])\n",
    "    return row   \n",
    "complete_df = rdd_complete.map(to_row).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
      "|customer_index|  f_0|  f_1|  f_2|  f_3|  f_4|  f_5|  f_6|  f_7|  f_8|  f_9| target|\n",
      "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
      "|             1|  0.5|  0.8|-0.49| 0.25| 0.37| 0.73|-0.43| 0.89|-1.85|-0.44|  36.67|\n",
      "|             3| 0.14|-0.87|-0.94| 0.09|-0.69|-0.29|-0.45| -0.6|-1.28|-0.38|-429.54|\n",
      "|             4|-0.79|-0.28|-0.26| 0.28|-0.17|-0.51| 0.44|-0.62| 1.82|-0.35| -18.23|\n",
      "|             5|-0.11| 0.86| -1.3|-0.09|-0.12|-0.47| 0.05| 0.98|-1.59|  0.3|  -5.52|\n",
      "|             9| 0.57| 0.07|-0.31|-0.92|-0.26| -0.9|-0.06|-0.76| 1.39| 0.01|-111.88|\n",
      "|            10|-0.89|  0.3| 0.62|-0.21|-1.02|-0.28| 0.54| 1.83|-0.35| 0.55| -34.48|\n",
      "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
      "|customer_index|  f_0|  f_1|  f_2|  f_3|  f_4|  f_5|  f_6|  f_7|  f_8|  f_9| target|\n",
      "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
      "|             1|  0.5|  0.8|-0.49| 0.25| 0.37| 0.73|-0.43| 0.89|-1.85|-0.44|  36.67|\n",
      "|             3| 0.14|-0.87|-0.94| 0.09|-0.69|-0.29|-0.45| -0.6|-1.28|-0.38|-429.54|\n",
      "|             4|-0.79|-0.28|-0.26| 0.28|-0.17|-0.51| 0.44|-0.62| 1.82|-0.35| -18.23|\n",
      "|             5|-0.11| 0.86| -1.3|-0.09|-0.12|-0.47| 0.05| 0.98|-1.59|  0.3|  -5.52|\n",
      "|             9| 0.57| 0.07|-0.31|-0.92|-0.26| -0.9|-0.06|-0.76| 1.39| 0.01|-111.88|\n",
      "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "rdd_complete.map(to_row).toDF().orderBy('customer_index').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2d8a63ee5a5324c0b86e37742630d9b7",
     "grade": true,
     "grade_id": "cell-a1f30aae3cf357e3",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# test that the creation of the row is successful\n",
    "assert rdd_complete.map(to_row).toDF()\n",
    "np.testing.assert_array_equal(rdd_complete.map(to_row).toDF().columns, \n",
    "              ['customer_index',\n",
    " 'f_0',\n",
    " 'f_1',\n",
    " 'f_2',\n",
    " 'f_3',\n",
    " 'f_4',\n",
    " 'f_5',\n",
    " 'f_6',\n",
    " 'f_7',\n",
    " 'f_8',\n",
    " 'f_9',\n",
    " 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.6 (10 pts):\n",
    "\n",
    "We will now use the `to_row` function created above to create a dataframe of the data `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e32c44443a35936770c76a2af3848e70",
     "grade": false,
     "grade_id": "cell-e81ad00df6011794",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = rdd_complete.map(to_row).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the dataframe a bit:\n",
    "\n",
    "```python\n",
    "df.orderBy('customer_index').show(5)\n",
    "```\n",
    "\n",
    "```console\n",
    "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
    "|customer_index|  f_0|  f_1|  f_2|  f_3|  f_4|  f_5|  f_6|  f_7|  f_8|  f_9| target|\n",
    "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
    "|             1|  0.5|  0.8|-0.49| 0.25| 0.37| 0.73|-0.43| 0.89|-1.85|-0.44|  36.67|\n",
    "|             3| 0.14|-0.87|-0.94| 0.09|-0.69|-0.29|-0.45| -0.6|-1.28|-0.38|-429.54|\n",
    "|             4|-0.79|-0.28|-0.26| 0.28|-0.17|-0.51| 0.44|-0.62| 1.82|-0.35| -18.23|\n",
    "|             5|-0.11| 0.86| -1.3|-0.09|-0.12|-0.47| 0.05| 0.98|-1.59|  0.3|  -5.52|\n",
    "|             9| 0.57| 0.07|-0.31|-0.92|-0.26| -0.9|-0.06|-0.76| 1.39| 0.01|-111.88|\n",
    "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
      "|customer_index|  f_0|  f_1|  f_2|  f_3|  f_4|  f_5|  f_6|  f_7|  f_8|  f_9| target|\n",
      "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
      "|             1|  0.5|  0.8|-0.49| 0.25| 0.37| 0.73|-0.43| 0.89|-1.85|-0.44|  36.67|\n",
      "|             3| 0.14|-0.87|-0.94| 0.09|-0.69|-0.29|-0.45| -0.6|-1.28|-0.38|-429.54|\n",
      "|             4|-0.79|-0.28|-0.26| 0.28|-0.17|-0.51| 0.44|-0.62| 1.82|-0.35| -18.23|\n",
      "|             5|-0.11| 0.86| -1.3|-0.09|-0.12|-0.47| 0.05| 0.98|-1.59|  0.3|  -5.52|\n",
      "|             9| 0.57| 0.07|-0.31|-0.92|-0.26| -0.9|-0.06|-0.76| 1.39| 0.01|-111.88|\n",
      "+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explore it yourself\n",
    "df.orderBy('customer_index').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subpackage `pyspark.ml.functions` (aliased as `fn` below) contains many common functions for data analysis. Find the function for computing the __correlation__ (the strength of the linear relationship between two variables) between two columns, the function for computing __absolute__ values, and create a data frame `correlations_df` that contains the following columns in the following order:\n",
    "\n",
    "1. `c0_target`: correlation between feature 0 and target\n",
    "1. `c1_target`: correlation between feature 1 and target\n",
    "1. `c2_target`: correlation between feature 2 and target\n",
    "1. `c3_target`: correlation between feature 3 and target\n",
    "1. `c4_target`: correlation between feature 4 and target\n",
    "1. `c5_target`: correlation between feature 5 and target\n",
    "1. `c6_target`: correlation between feature 6 and target\n",
    "1. `c7_target`: correlation between feature 7 and target\n",
    "1. `c8_target`: correlation between feature 8 and target\n",
    "1. `c9_target`: correlation between feature 9 and target\n",
    "1. `sig0`: boolean `true` if the absolute value of the correlation between feature 0 and target is greater than 0.5, `false` o.w.\n",
    "1. `sig1`: boolean `true` if the absolute value of the correlation between feature 1 and target is greater than 0.5, `false` o.w.\n",
    "1. `sig2`: boolean `true` if the absolute value of the correlation between feature 2 and target is greater than 0.5, `false` o.w.\n",
    "1. `sig3`: boolean `true` if the absolute value of the correlation between feature 3 and target is greater than 0.5, `false` o.w.\n",
    "1. `sig4`: boolean `true` if the absolute value of the correlation between feature 4 and target is greater than 0.5, `false` o.w.\n",
    "1. `sig5`: boolean `true` if the absolute value of the correlation between feature 5 and target is greater than 0.5, `false` o.w.\n",
    "1. `sig6`: boolean `true` if the absolute value of the correlation between feature 6 and target is greater than 0.5, `false` o.w.\n",
    "1. `sig7`: boolean `true` if the absolute value of the correlation between feature 7 and target is greater than 0.5, `false` o.w.\n",
    "1. `sig8`: boolean `true` if the absolute value of the correlation between feature 8 and target is greater than 0.5, `false` o.w.\n",
    "1. `sig9`: boolean `true` if the absolute value of the correlation between feature 9 and target is greater than 0.5, `false` o.w.\n",
    "\n",
    "**Hint: Remember that you can pass a list of columns to `df.select`. You can create such list with list comprehension, saving a lot of code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the package functions as fn\n",
    "from pyspark.sql import functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[df.corr(df.columns[i],'target','pearson') for i in range(1,len(df.columns)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.004550467203235587,\n",
       " 0.5175418076531234,\n",
       " 0.2422254992218165,\n",
       " -0.027252922956860874,\n",
       " 0.6109343603342671,\n",
       " -0.015061887057095293,\n",
       " 0.5763480950714734,\n",
       " 0.06334859042481795,\n",
       " 0.007563889344988282,\n",
       " -0.01715842226040011]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a)):\n",
    "    if a[i]>0.5:\n",
    "        a.append(True)\n",
    "    else:\n",
    "        a.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.004550467203235587,\n",
       " 0.5175418076531234,\n",
       " 0.2422254992218165,\n",
       " -0.027252922956860874,\n",
       " 0.6109343603342671,\n",
       " -0.015061887057095293,\n",
       " 0.5763480950714734,\n",
       " 0.06334859042481795,\n",
       " 0.007563889344988282,\n",
       " -0.01715842226040011,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d50d7ffa26caed36ab9594774d715b63",
     "grade": false,
     "grade_id": "cell-12e8f5f6de39ae69",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the dataframe `correlations_df` here\n",
    "a=tuple(a)\n",
    "correlations_df=spark.createDataFrame([a],['c0_target','c1_target','c2_target','c3_target','c4_target','c5_target','c6_target','c7_target','c8_target','c9_target','sig0','sig1','sig2','sig3','sig4','sig5','sig6','sig7','sig8','sig9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86a41f3db733560238662ea32ca87d16",
     "grade": true,
     "grade_id": "cell-ea3814c44cba8dcc",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the dataframe has the correct columns and values. There could be hidden tests!\"\"\"\n",
    "# check column names\n",
    "column_names = ['c' + str(fi) + '_target' for fi in range(10)] + \\\n",
    "               ['sig' + str(fi) for fi in range(10)]\n",
    "\n",
    "# column's names and positions in the right order\n",
    "np.testing.assert_equal(correlations_df.columns, column_names)\n",
    "\n",
    "from pyspark.sql.types import DoubleType, BooleanType\n",
    "\n",
    "\n",
    "# the types are correct\n",
    "np.testing.assert_equal([type(f.dataType) for f in correlations_df.schema.fields], \n",
    "                        10*[DoubleType] + 10*[BooleanType])\n",
    "\n",
    "# the values are correct\n",
    "np.testing.assert_approx_equal(correlations_df.toPandas().sum().sum(), \n",
    "                               4.963039476979365,\n",
    "                               significant=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grading Feedback Cell"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
